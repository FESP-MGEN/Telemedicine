{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "#####################################################################################################\n",
    "# 1_Functions\n",
    "#####################################################################################################\n",
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a function that retrieves all the indexes of a character string in a text file\n",
    "def all_occurences(file, str):\n",
    "    initial = 0\n",
    "    while True:\n",
    "        initial = file.find(str, initial)\n",
    "        if initial == -1: return\n",
    "        yield initial\n",
    "        initial += len(str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function for text file from Zotero software\n",
    "def import_zot(file):\n",
    "    file = open(file,'r', encoding=\"utf-8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    # We put the text entirely in lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # Creation of a list indicating the words to delete\n",
    "    listeMotsASupprimer = ['{\\\\textgreater}','{\\\\textless}']\n",
    "\n",
    "    # For each word from the list: \"listeMotsASupprimer\", python will store it in the variable \n",
    "    # \"motASupprimer\", and do the replacement processing with this word\n",
    "    for motASupprimer in listeMotsASupprimer:\n",
    "        text=text.replace(motASupprimer,'') # no space to not distort the text\n",
    "    \n",
    "    # Calculation of the number of articles in the input text file\n",
    "    nb_article=0\n",
    "    nb_article=text.count('@article')\n",
    "    \n",
    "    # Creation of a list containing all the indexes at the start of the article block\n",
    "    liste_index_art=list(all_occurences(text,'@article'))\n",
    "    \n",
    "    # Creation of an article list which contains an article block for each element of the list\n",
    "    article=[]\n",
    "    for i in range(nb_article-1):\n",
    "        s=text[liste_index_art[i]:liste_index_art[i+1]]\n",
    "        article.append(s)\n",
    "    # We add the last abstract\n",
    "    article.append(text[liste_index_art[nb_article-1]:])\n",
    "\n",
    "    # Isolation of the backgrounds then creation of an abstract list, with an abstract by element \n",
    "    # or \"abstract absent\" in the case where there is no abstract\n",
    "    index_acf=0\n",
    "    index_ac=0\n",
    "    abstract=[]\n",
    "\n",
    "    for a in article:\n",
    "        index_a=a.find('abstract =', )\n",
    "        index_ac=a.find('{',index_a,)\n",
    "        index_acf=a.find('}', index_ac,)\n",
    "        if index_a!=-1:\n",
    "            t=a[index_ac+1:index_acf]\n",
    "            abstract.append(t)\n",
    "        else:\n",
    "            abstract.append(\"absent abstract\")              \n",
    "    return abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function for text files from Cochrane\n",
    "def import_coch(file):\n",
    "    file = open(file,'r', encoding=\"utf-8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    # We put the text entirely in lowercase\n",
    "    text=text.lower()\n",
    "\n",
    "    # Calculation of the number of articles in the input text file\n",
    "    nb_article=0\n",
    "    nb_article=text.count('record #')\n",
    "\n",
    "    # Creation of a list containing all the indexes at the start of the article block\n",
    "    liste_index_art=list(all_occurences(text,'record #'))\n",
    "\n",
    "    # Creation of an article list which contains an article block for each element of the list\n",
    "    article=[]\n",
    "    for i in range(nb_article-1):\n",
    "        s=text[liste_index_art[i]:liste_index_art[i+1]]\n",
    "        article.append(s)\n",
    "    # We add the larst abstract\n",
    "    article.append(text[liste_index_art[nb_article-1]:])\n",
    "\n",
    "    # Isolation of the backgrounds then creation of an abstract list, with an abstract by element \n",
    "    # or \"abstract absent\" in the case where there is no abstract\n",
    "    index_acf=0\n",
    "    index_ac=0\n",
    "    abstract=[]\n",
    "\n",
    "    for a in article:\n",
    "        index_a=a.find('ab: ', )+2\n",
    "        index_acf=a.find('us: ', index_a,)\n",
    "        if index_a!=-1:\n",
    "            t=a[index_a+1:index_acf]\n",
    "            abstract.append(t)\n",
    "\n",
    "        else:\n",
    "            abstract.append(\"absent abstract\")\n",
    "    return abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a clean function to clean all abstracts\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which generates n-gramm from a list of words\n",
    "def generate_ngrams(words_list, n):\n",
    "    ngrams_list = []\n",
    " \n",
    "    for num in range(0, len(words_list)):\n",
    "        ngram = ' '.join(words_list[num:num + n])\n",
    "        ngrams_list.append(ngram) \n",
    "    return ngrams_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a function that takes as input a list of words and a list of keywords to find\n",
    "# The function will output a set (without duplicates) of keywords\n",
    "def recherche_mot_cle(liste_mot, liste_mot_cle):\n",
    "    select=[]\n",
    "       \n",
    "    for mot in liste_mot:\n",
    "        if mot in liste_mot_cle:\n",
    "            select.append(mot)\n",
    "    # We transform the list as a whole which allows us to remove duplicates\n",
    "    ens=set(select)           \n",
    "    return ens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are looking to enrich the word list of each abstract with n-grams\n",
    "def gen_liste_ngrams(doc,n):\n",
    "    liste_ngram=[]\n",
    "    for i in range(1,n+1):\n",
    "        liste_ngram=liste_ngram+(generate_ngrams(doc,i))        \n",
    "    return liste_ngram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for creating a word cloud\n",
    "\n",
    "# 1/ For single word:\n",
    "# we enter in parameter the list and the number of words that we want to appear\n",
    "def wordcloud(liste, n, color):\n",
    "    words_dict=dict(nltk.FreqDist(liste))\n",
    "    # Generate word clouds and save them as jpg images\n",
    "    WC_height = 2000\n",
    "    WC_width = 3000\n",
    "    WC_max_words = n\n",
    "    wordCloud = WordCloud(max_font_size=750,max_words=WC_max_words, height=WC_height, width=WC_width, background_color=\"white\",  colormap=color)\n",
    "    wordCloud.generate_from_frequencies(words_dict)\n",
    "    plt.imshow(wordCloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return wordCloud\n",
    "\n",
    "\n",
    "# 2/ For bigrams, you enter in parameter the list and the number of words you want to appear\n",
    "def wordcloud_bigram(liste, n, color):\n",
    "    # Calculation of bigrams with the nltk and countVectorizer function\n",
    "    bigrams_list = list(nltk.bigrams(liste))\n",
    "    dictionary2 = [' '.join(tup) for tup in bigrams_list]\n",
    "    words_dict=dict(nltk.FreqDist(dictionary2))\n",
    "\n",
    "    # Generate word clouds and save them as jpg images\n",
    "    WC_height = 2000\n",
    "    WC_width = 3000\n",
    "    WC_max_words = n\n",
    "    wordCloud = WordCloud(max_font_size=750,max_words=WC_max_words, height=WC_height, width=WC_width, background_color=\"white\",  colormap=color)\n",
    "    wordCloud.generate_from_frequencies(words_dict)\n",
    "    plt.imshow(wordCloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return wordCloud\n",
    "    \n",
    "# 3/ For trigrams, enter the list and the number of words you want to appear in parameters\n",
    "def wordcloud_trigram(liste, n, color):\n",
    "    trigrams_list = list(nltk.trigrams(liste))\n",
    "    dictionary2 = [' '.join(tup) for tup in trigrams_list]\n",
    "    words_dict=dict(nltk.FreqDist(dictionary2))\n",
    "\n",
    "    # Generate word clouds and save them as jpg images\n",
    "    WC_height = 2000\n",
    "    WC_width = 3000\n",
    "    WC_max_words = n\n",
    "    wordCloud = WordCloud(max_font_size=750,max_words=WC_max_words, height=WC_height, width=WC_width, background_color=\"white\",  colormap=color)\n",
    "    wordCloud.generate_from_frequencies(words_dict)\n",
    "    plt.imshow(wordCloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return wordCloud\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
